{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "fBT7wpLMZI4D"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('DocMeta2.tsv') as f:\n",
    "    data=f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 14048,
     "status": "ok",
     "timestamp": 1553693641637,
     "user": {
      "displayName": "武楚涵",
      "photoUrl": "",
      "userId": "13980017009180601715"
     },
     "user_tz": -480
    },
    "id": "tavc7qCaw78H",
    "outputId": "d768fdbe-12e4-4551-88f7-019d6c3df3ec"
   },
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "news={}\n",
    "category={'none':0}\n",
    "subcategory={'none':0}\n",
    "for i in data:\n",
    "    tp=i.strip().split('\\t')\n",
    "    if len(tp)<5:\n",
    "        continue\n",
    "    news[tp[1]]=[tp[2],tp[3],word_tokenize(tp[6].lower())]\n",
    "    if tp[2] not in category:\n",
    "        category[tp[2]]=len(category)\n",
    "    if tp[3] not in subcategory:\n",
    "        subcategory[tp[3]]=len(subcategory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "AhddvsR-Jyb2"
   },
   "outputs": [],
   "source": [
    "with open('ClickData2.tsv')as f:\n",
    "    user=f.readlines()\n",
    "userid_dict={}\n",
    "for i in user:\n",
    "    tr=i.strip().split('\\t')\n",
    "    userid=tr[0]\n",
    "    if userid not in  userid_dict:\n",
    "        userid_dict[userid]=len(userid_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('userid.txt','r')as f:\n",
    "    rawuserlabel=[x.strip().split('\\t') for x in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "userlabeldict={}\n",
    "genderdict={'M':0,'F':1}\n",
    "\n",
    "for i in rawuserlabel:\n",
    "    if len(i)==2:\n",
    "        userlabeldict[i[0]]=[0,0]\n",
    "    else:\n",
    "        userlabeldict[i[0]]=[genderdict[i[2]],1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in userid_dict:\n",
    "    if i not in userlabeldict:\n",
    "        userlabeldict[i]=[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "rFc2aJNBCjUj"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "word_dict0={'PADDING':[0,999999]}\n",
    "\n",
    "for i in news:\n",
    "    for j in news[i][2]:\n",
    "        if j in word_dict0:\n",
    "            word_dict0[j][1]+=1\n",
    "        else:\n",
    "            word_dict0[j]=[len(word_dict0),1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 202640,
     "status": "ok",
     "timestamp": 1553693859000,
     "user": {
      "displayName": "武楚涵",
      "photoUrl": "",
      "userId": "13980017009180601715"
     },
     "user_tz": -480
    },
    "id": "I-Cem70S3wQM",
    "outputId": "49c818ed-9523-4bbd-d6c7-674ac0d36d25"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "word_dict={}\n",
    "for i in word_dict0:\n",
    "    if word_dict0[i][1]>=1:\n",
    "        word_dict[i]=[len(word_dict),word_dict0[i][1]]\n",
    "print(len(word_dict),len(word_dict0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "wbqy_7jaImAn"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "embdict={}\n",
    "plo=0\n",
    "import pickle\n",
    "with open('/data/wuch/glove.840B.300d.txt','rb')as f:\n",
    "    linenb=0\n",
    "    while True:\n",
    "        j=f.readline()\n",
    "        if len(j)==0:\n",
    "            break\n",
    "        k = j.split()\n",
    "        word=k[0].decode()\n",
    "        linenb+=1\n",
    "        if len(word) != 0:\n",
    "            tp=[float(x) for x in k[1:]]\n",
    "            if word in word_dict:\n",
    "                embdict[word]=tp\n",
    "                if plo%100==0:\n",
    "                    print(plo,linenb,word)\n",
    "                plo+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 188757,
     "status": "ok",
     "timestamp": 1553657135244,
     "user": {
      "displayName": "武楚涵",
      "photoUrl": "",
      "userId": "13980017009180601715"
     },
     "user_tz": -480
    },
    "id": "qijHN2JH-bGK",
    "outputId": "b6a021d8-a60e-47d3-af28-7eed6a7a5377"
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import cholesky\n",
    "word_dict1=word_dict\n",
    "print(len(embdict),len(word_dict1))\n",
    "print(len(word_dict1))\n",
    "lister=[0]*len(word_dict1)\n",
    "xp=np.zeros(300,dtype='float32')\n",
    "\n",
    "cand=[]\n",
    "for i in embdict.keys():\n",
    "    lister[word_dict1[i][0]]=np.array(embdict[i],dtype='float32')\n",
    "    cand.append(lister[word_dict1[i][0]])\n",
    "cand=np.array(cand,dtype='float32')\n",
    "\n",
    "mu=np.mean(cand, axis=0)\n",
    "Sigma=np.cov(cand.T)\n",
    "\n",
    "norm=np.random.multivariate_normal(mu, Sigma, 1)\n",
    "print(mu.shape,Sigma.shape,norm.shape)\n",
    "\n",
    "for i in range(len(lister)):\n",
    "    if type(lister[i])==int:\n",
    "        lister[i]=np.reshape(norm, 300)\n",
    "lister[0]=np.zeros(300,dtype='float32')\n",
    "lister=np.array(lister,dtype='float32')\n",
    "print(lister.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "news_words=[[0]*30]\n",
    "news_index={'0':0}\n",
    "for i in news:\n",
    "    tp=[]\n",
    "    news_index[i]=len(news_index)\n",
    "    for j in news[i][2]:\n",
    "        if j in word_dict:\n",
    "            tp.append(word_dict[j][0])\n",
    "    tp=tp[:30]\n",
    "    news_words.append(tp+[0]*(30-len(tp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random\n",
    "import random\n",
    "def newsample(neglist,ratio):\n",
    "    if ratio >len(neglist):\n",
    "        return random.sample(neglist*(ratio//len(neglist)+1),ratio)\n",
    "    else:\n",
    "        return random.sample(neglist,ratio)\n",
    "npratio=4\n",
    "all_train_id=[]\n",
    "all_train_pn=[]    \n",
    "all_labeler=[]\n",
    "\n",
    "all_test_id=[]\n",
    "all_test_pn=[]    \n",
    "all_test_labeler=[]\n",
    "all_test_index=[]\n",
    "\n",
    "all_user_pos=[]\n",
    "all_test_user_pos=[]\n",
    "\n",
    "for line in user:\n",
    "    linesplit=line.strip().split('\\t')\n",
    "    userid=linesplit[0]\n",
    "\n",
    "    impre=[x.split('#TAB#') for x in linesplit[2].split('#N#')]\n",
    "\n",
    "    trainpos=[x[0].split() for x in impre]\n",
    "    trainneg=[x[1].split() for x in impre]\n",
    "     \n",
    "    poslist=list(itertools.chain(*(trainpos)))\n",
    "    neglist=list(itertools.chain(*(trainneg)))\n",
    "    #print(poslist)\n",
    "    \n",
    "    \n",
    "    if len(linesplit)==4:\n",
    "        impre=[x.split('#TAB#') for x in linesplit[3].split('#N#')]\n",
    "        testpos=[x[0].split() for x in impre]\n",
    "        testneg=[x[1].split() for x in impre]\n",
    "        \n",
    "        \n",
    "        for i in range(len(testpos)):\n",
    "            sample_index=[]\n",
    "            sample_index.append(len(all_test_pn))\n",
    "            posset=list(set(poslist))\n",
    "            allpos=[news_index[p] for p in random.sample(posset,min(50,len(posset)))[:50]]\n",
    "            allpos+=[0]*(50-len(allpos))\n",
    "    \n",
    "            \n",
    "            for j in testpos[i]:\n",
    "                all_test_pn.append(news_index[j])\n",
    "                all_test_labeler.append(1)\n",
    "                all_test_id.append(userid_dict[userid])\n",
    "                all_test_user_pos.append(allpos)\n",
    "                \n",
    "            for j in testneg[i]:\n",
    "                all_test_pn.append(news_index[j])\n",
    "                all_test_labeler.append(0)\n",
    "                all_test_id.append(userid_dict[userid])\n",
    "                all_test_user_pos.append(allpos)\n",
    "            sample_index.append(len(all_test_pn))\n",
    "            all_test_index.append(sample_index)\n",
    "            \n",
    "    #neglistnn=list(set(neglist)-set(poslist))\n",
    "\n",
    "            \n",
    "    for impre_id in range(len(trainpos)):\n",
    "        for pos_sample in trainpos[impre_pos_id]:\n",
    "            #tql=list(set(neglist)-set(trainpos[mp]))\n",
    "            all_sample=newsample(trainneg[impre_id],npratio)\n",
    "            all_sample.append(pos_sample)\n",
    "            templabel=[0]*npratio+[1]\n",
    "            labelid=list(range(npratio+1))\n",
    "            random.shuffle(labelid)\n",
    "\n",
    "            \n",
    "            sample=[]\n",
    "            label=[]\n",
    "            for j in labelid:\n",
    "                sample.append(news_index[all_sample[j]])\n",
    "                label.append(templabel[j])\n",
    "            \n",
    "            posset=list(set(poslist)-set([pos_sample]))\n",
    "            allpos=[news_index[p] for p in random.sample(posset,min(50,len(posset)))[:50]]\n",
    "            allpos+=[0]*(50-len(allpos))\n",
    "            all_train_pn.append(yp)\n",
    "            all_labeler.append(yplb)\n",
    "            all_train_id.append(userid_dict[userid])\n",
    "            all_user_pos.append(allpos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inverseuserid_dic={userid_dict[x]:x for x in userid_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "news_words=np.array(news_words,dtype='int32') \n",
    "\n",
    "all_train_pn=np.array(all_train_pn,dtype='int32')\n",
    "all_labeler=np.array(all_labeler,dtype='int32')\n",
    "all_train_id=np.array(all_train_id,dtype='int32')\n",
    "all_test_pn=np.array(all_test_pn,dtype='int32')\n",
    "all_test_labeler=np.array(all_test_labeler,dtype='int32')\n",
    "all_test_id=np.array(all_test_id,dtype='int32')\n",
    "all_user_pos=np.array(all_user_pos,dtype='int32')\n",
    "all_test_user_pos=np.array(all_test_user_pos,dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_train_gender_label=np.array([userlabeldict[inverseuserid_dic[x]] for x in all_train_id],dtype='int32') \n",
    "all_test_gender_label=np.array([userlabeldict[inverseuserid_dic[x]] for x in all_test_id],dtype='int32') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "npratio=4   \n",
    "user_label_gender=[]\n",
    "user_pos=[]\n",
    "\n",
    "for line in user:\n",
    "    linesplit=line.strip().split('\\t')\n",
    "    userid=linesplit[0]\n",
    "\n",
    "    impre=[x.split('#TAB#') for x in linesplit[2].split('#N#')]\n",
    "\n",
    "    trainpos=[x[0].split() for x in impre]\n",
    "     \n",
    "    poslist=list(itertools.chain(*(trainpos)))\n",
    "    if userid not in userlabeldict or userlabeldict[userid][1]==0:\n",
    "        continue\n",
    "     \n",
    "            \n",
    "    allpos=[news_index[p] for p in random.sample(poslist,min(50,len(poslist)))[:50]]\n",
    "    allpos+=[0]*(50-len(allpos))\n",
    "\n",
    "    user_label_gender.append(userlabeldict[userid][0])\n",
    "    user_pos.append(allpos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_pos_words=news_words[np.array(user_pos,dtype='int32')]\n",
    "user_label_gender=np.array(user_label_gender,dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 167770,
     "status": "ok",
     "timestamp": 1553693859018,
     "user": {
      "displayName": "武楚涵",
      "photoUrl": "",
      "userId": "13980017009180601715"
     },
     "user_tz": -480
    },
    "id": "-PIqsLsPV1aH",
    "outputId": "c2d86e15-034e-4178-ace8-38f104d78950"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.models import Model, load_model\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "from keras import initializers #keras2\n",
    "from keras.utils import plot_model\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from keras.optimizers import *\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_batch_data_random(all_train_pn,all_labeler,all_train_id,batch_size):\n",
    "    idx = np.arange(len(all_labeler))\n",
    "    np.random.shuffle(idx)\n",
    "    y=all_labeler\n",
    "    batches = [idx[range(batch_size*i, min(len(y), batch_size*(i+1)))] for i in range(len(y)//batch_size+1)]\n",
    "\n",
    "    while (True):\n",
    "        for i in batches:\n",
    "            itx = news_words[all_train_pn[i]]\n",
    "            usx=news_words[all_user_pos[i]]\n",
    "            yy=all_labeler[i]\n",
    "            \n",
    "            yield ([itx,usx], [yy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_batch_data_random_gender(all_train_pn,all_labeler,all_train_id,batch_size):\n",
    "    idx = np.arange(len(all_labeler))\n",
    "    np.random.shuffle(idx)\n",
    "    y=all_labeler\n",
    "    batches = [idx[range(batch_size*i, min(len(y), batch_size*(i+1)))] for i in range(len(y)//batch_size+1)]\n",
    "\n",
    "    while (True):\n",
    "        for i in batches:\n",
    "            itx = news_words[all_train_pn[i]]\n",
    "            usx=news_words[all_user_pos[i]]\n",
    "            yy=all_labeler[i]\n",
    "            yg=all_train_gender_label[:,0][i]\n",
    "            ygmask=all_train_gender_label[:,1][i]\n",
    "            \n",
    "            yield ([itx,usx], [yy,yg,np.zeros((batch_size,),dtype='float32')],[np.ones((batch_size,),dtype='float32'),ygmask,np.ones((batch_size,),dtype='float32')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_batch_data(all_train_pn,all_labeler,all_train_id,batch_size):\n",
    "    idx = np.arange(len(all_labeler))\n",
    "    y=all_labeler\n",
    "    batches = [idx[range(batch_size*i, min(len(y), batch_size*(i+1)))] for i in range(len(y)//batch_size+1)]\n",
    "\n",
    "    while (True):\n",
    "        for i in batches:\n",
    "            itx = news_words[all_train_pn[i]]\n",
    "\n",
    "            usx=news_words[all_test_user_pos[i]]\n",
    "            yy=all_labeler[i]\n",
    "            yield ([itx,usx], [yy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def dcg_score(y_true, y_score, k=10):\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])\n",
    "    gains = 2 ** y_true - 1\n",
    "    discounts = np.log2(np.arange(len(y_true)) + 2)\n",
    "    return np.sum(gains / discounts)\n",
    "\n",
    "\n",
    "def ndcg_score(y_true, y_score, k=10):\n",
    "    best = dcg_score(y_true, y_true, k)\n",
    "    actual = dcg_score(y_true, y_score, k)\n",
    "    return actual / best\n",
    "\n",
    "\n",
    "def mrr_score(y_true, y_score):\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order)\n",
    "    rr_score = y_true / (np.arange(len(y_true)) + 1)\n",
    "    return np.sum(rr_score) / np.sum(y_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from keras.layers import *\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "\n",
    "    def __init__(self, nb_head, size_per_head, **kwargs):\n",
    "        self.nb_head = nb_head\n",
    "        self.size_per_head = size_per_head\n",
    "        self.output_dim = nb_head*size_per_head\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.WQ = self.add_weight(name='WQ', \n",
    "                                  shape=(input_shape[0][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        self.WK = self.add_weight(name='WK', \n",
    "                                  shape=(input_shape[1][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        self.WV = self.add_weight(name='WV', \n",
    "                                  shape=(input_shape[2][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    "        \n",
    "    def Mask(self, inputs, seq_len, mode='mul'):\n",
    "        if seq_len == None:\n",
    "            return inputs\n",
    "        else:\n",
    "            mask = K.one_hot(seq_len[:,0], K.shape(inputs)[1])\n",
    "            mask = 1 - K.cumsum(mask, 1)\n",
    "            for _ in range(len(inputs.shape)-2):\n",
    "                mask = K.expand_dims(mask, 2)\n",
    "            if mode == 'mul':\n",
    "                return inputs * mask\n",
    "            if mode == 'add':\n",
    "                return inputs - (1 - mask) * 1e12\n",
    "                \n",
    "    def call(self, x):\n",
    "        #如果只传入Q_seq,K_seq,V_seq，那么就不做Mask\n",
    "        #如果同时传入Q_seq,K_seq,V_seq,Q_len,V_len，那么对多余部分做Mask\n",
    "        if len(x) == 3:\n",
    "            Q_seq,K_seq,V_seq = x\n",
    "            Q_len,V_len = None,None\n",
    "        elif len(x) == 5:\n",
    "            Q_seq,K_seq,V_seq,Q_len,V_len = x\n",
    "        #对Q、K、V做线性变换\n",
    "        Q_seq = K.dot(Q_seq, self.WQ)\n",
    "        Q_seq = K.reshape(Q_seq, (-1, K.shape(Q_seq)[1], self.nb_head, self.size_per_head))\n",
    "        Q_seq = K.permute_dimensions(Q_seq, (0,2,1,3))\n",
    "        K_seq = K.dot(K_seq, self.WK)\n",
    "        K_seq = K.reshape(K_seq, (-1, K.shape(K_seq)[1], self.nb_head, self.size_per_head))\n",
    "        K_seq = K.permute_dimensions(K_seq, (0,2,1,3))\n",
    "        V_seq = K.dot(V_seq, self.WV)\n",
    "        V_seq = K.reshape(V_seq, (-1, K.shape(V_seq)[1], self.nb_head, self.size_per_head))\n",
    "        V_seq = K.permute_dimensions(V_seq, (0,2,1,3))\n",
    "        #计算内积，然后mask，然后softmax\n",
    "        A = K.batch_dot(Q_seq, K_seq, axes=[3,3]) / self.size_per_head**0.5\n",
    "        A = K.permute_dimensions(A, (0,3,2,1))\n",
    "        A = self.Mask(A, V_len, 'add')\n",
    "        A = K.permute_dimensions(A, (0,3,2,1))    \n",
    "        A = K.softmax(A)\n",
    "        #输出并mask\n",
    "        O_seq = K.batch_dot(A, V_seq, axes=[3,2])\n",
    "        O_seq = K.permute_dimensions(O_seq, (0,2,1,3))\n",
    "        O_seq = K.reshape(O_seq, (-1, K.shape(O_seq)[1], self.output_dim))\n",
    "        O_seq = self.Mask(O_seq, Q_len, 'mul')\n",
    "        return O_seq\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0][0], input_shape[0][1], self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import random\n",
    "\n",
    "from keras import objectives\n",
    "results=[]\n",
    " \n",
    "keras.backend.clear_session()\n",
    "\n",
    "MAX_SENT_LENGTH=30\n",
    "MAX_SENTS=50\n",
    "\n",
    "\n",
    "sentence_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "\n",
    "embedding_layer = Embedding(len(word_dict), 300, weights=[lister],trainable=True)\n",
    "\n",
    "embedded_sequences = embedding_layer(sentence_input)\n",
    "embedded_sequences=Dropout(0.2)(embedded_sequences)\n",
    "\n",
    "#bias-aware\n",
    "\n",
    "word_rep =Attention(16,16)([embedded_sequences,embedded_sequences,embedded_sequences])\n",
    "word_rep=Dropout(0.2)(word_rep)\n",
    "\n",
    "attention_word = Dense(200,activation='tanh')(word_rep)\n",
    "attention_word = Flatten()(Dense(1)(attention_word))\n",
    "attention_word = Activation('softmax')(attention_word)\n",
    "news_rep=keras.layers.Dot((1, 1))([word_rep, attention_word])\n",
    "\n",
    "\n",
    "\n",
    "news_encoder = Model([sentence_input], news_rep)\n",
    "\n",
    "\n",
    "history_input = keras.Input((MAX_SENTS,MAX_SENT_LENGTH,), dtype='int32') \n",
    "his_vecs =TimeDistributed(news_encoder)(history_input)\n",
    "\n",
    "his_vecs=Attention(16,16)([his_vecs,his_vecs,his_vecs])\n",
    "attention_news = Dense(200,activation='tanh')(his_vecs)\n",
    "attention_news =  Flatten()(Dense(1)(attention_news))\n",
    "attention_news = Activation('softmax')(attention_news)\n",
    "user_emb=keras.layers.Dot((1, 1))([his_vecs, attention_news])\n",
    "\n",
    "#bias-free\n",
    "\n",
    "his_vecs2=Attention(16,16)([his_vecs,his_vecs,his_vecs])\n",
    "attention_news2 = Dense(200,activation='tanh')(his_vecs2)\n",
    "attention_news2 =  Flatten()(Dense(1)(attention_news2))\n",
    "attention_news2 = Activation('softmax')(attention_news2)\n",
    "user_emb2=keras.layers.Dot((1, 1))([his_vecs2, attention_news2])\n",
    "\n",
    "\n",
    "\n",
    "candidates = keras.Input((1+npratio,MAX_SENT_LENGTH,), dtype='int32') \n",
    "candidate_vecs = TimeDistributed(news_encoder)(candidates)\n",
    "\n",
    "user_emb_unified=add([user_emb,user_emb2])\n",
    "\n",
    "logits = keras.layers.dot([user_emb_unified, candidate_vecs], axes=-1)\n",
    "logits = keras.layers.Activation(keras.activations.softmax)(logits)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "loss_orth=Lambda(lambda x:K.batch_dot(x[0],x[1],axes=-1)/K.sqrt(K.sum(K.square(x[0]),axis=1)*K.sum(K.square(x[1]),axis=1)))([user_emb,user_emb2])\n",
    "def orthloss(y_true, y_pred):\n",
    "    return K.mean(K.abs(loss_orth))\n",
    "\n",
    "outattribute=Dense(1,activation='sigmoid')(Dense(128)(l_att2))\n",
    "\n",
    "model = Model([candidates,history_input], [logits,outattribute,loss_orth])\n",
    "model.trainable=True\n",
    "\n",
    "candidate_one = keras.Input((MAX_SENT_LENGTH,))\n",
    "candidate_one_vec = news_encoder([candidate_one])\n",
    "\n",
    "score = keras.layers.Activation(keras.activations.sigmoid)(\n",
    "    keras.layers.dot([user_emb2, candidate_one_vec], axes=-1))\n",
    "\n",
    "model_bias_free = keras.Model([candidate_one,history_input], score)\n",
    "\n",
    "\n",
    "\n",
    "model.compile(loss=['categorical_crossentropy','binary_crossentropy',orthloss],loss_weights=[1.,0.5,0.5], \n",
    "              optimizer=Adam(lr=0.0001), metrics=['acc'])\n",
    "\n",
    "\n",
    "\n",
    "usermodel = Model([candidates,history_input], [user_emb2])\n",
    " \n",
    "userv=Input((256,),dtype='float32')\n",
    "out=Dense(1,activation='sigmoid')(Dense(256)(userv))\n",
    "\n",
    "usergenderdis = Model([userv], [out])\n",
    "\n",
    "gan_output = usergenderdis(usermodel([candidates,history_input]))\n",
    "\n",
    "gan = keras.models.Model([candidates,history_input], [logits,gan_output,loss_orth])\n",
    "gan.compile(optimizer=Adam(lr=0.0001), metrics=['acc'],loss=['categorical_crossentropy','binary_crossentropy',orthloss],loss_weights=[1.,0.5,0.5])\n",
    "usergenderdis.compile(loss=['binary_crossentropy'], optimizer=Adam(lr=0.0001), metrics=['acc'])\n",
    "for epoch in range(1):\n",
    "    traingen=generate_batch_data_random_gender(all_train_pn,all_labeler,all_train_id, 30)\n",
    "    iterations=len(all_train_id)//30\n",
    "    step=0\n",
    "    historicala=[]\n",
    "    historicald=[]\n",
    "    for data in traingen:\n",
    "        disvec=usermodel.predict(data[0])\n",
    "        usergenderdis.trainable = True\n",
    "        d_loss =usergenderdis.train_on_batch(disvec,[1-data[1][1]],sample_weight=data[2][1])\n",
    "        usergenderdis.trainable = False\n",
    "        a_loss =gan.train_on_batch(data[0],[data[1][0],data[1][1],data[1][2]],sample_weight=data[2])\n",
    "        step+=1\n",
    "        historicald.append(d_loss)\n",
    "        historicala.append(a_loss)\n",
    "        historicald=historicald[-10:]\n",
    "        historicala=historicala[-10:]\n",
    "        if step%10==0:\n",
    "            print(d_loss)\n",
    "            print(np.mean(historicald,axis=0))\n",
    "            print(a_loss)\n",
    "            print(np.mean(historicala,axis=0))\n",
    "        if step==iterations:\n",
    "            break\n",
    "    valgen=generate_batch_data(all_test_pn,all_test_labeler,all_test_id, 94)\n",
    "    cr = model_bias_free.predict_generator(valgen, steps=len(all_test_id)//94,verbose=1)\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    all_auc=[]\n",
    "    all_mrr=[]\n",
    "    all_ndcg=[]\n",
    "    all_ndcg2=[]\n",
    "    for m in all_test_index:\n",
    "        if np.sum(all_test_labeler[m[0]:m[1]])!=0 and m[1]<len(cr):\n",
    "    \n",
    "            all_auc.append(roc_auc_score(all_test_labeler[m[0]:m[1]],cr[m[0]:m[1],0]))\n",
    "            all_mrr.append(mrr_score(all_test_labeler[m[0]:m[1]],cr[m[0]:m[1],0]))\n",
    "            all_ndcg.append(ndcg_score(all_test_labeler[m[0]:m[1]],cr[m[0]:m[1],0],k=5))\n",
    "            all_ndcg2.append(ndcg_score(all_test_labeler[m[0]:m[1]],cr[m[0]:m[1],0],k=10))\n",
    "    results.append([np.mean(all_auc),np.mean(all_mrr),np.mean(all_ndcg),np.mean(all_ndcg2)])\n",
    "    print(np.mean(all_auc),np.mean(all_mrr),np.mean(all_ndcg),np.mean(all_ndcg2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "userindex=np.array(random.sample(np.where(user_label_gender==0)[0].tolist(),len(np.where(user_label_gender==1)[0]))+np.where(user_label_gender==1)[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modeluser = Model([history_input], [user_emb])\n",
    "\n",
    "useremb=modeluser.predict(user_pos_words[userindex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modeluser2 = Model([history_input], [user_emb2])\n",
    "\n",
    "useremb2=modeluser2.predict(user_pos_words[userindex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newsemb=news_encoder.predict(news_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gender_index=np.arange(len(useremb))\n",
    "np.random.shuffle(gender_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k=10\n",
    "topnews=news_words[np.dot(useremb2,np.transpose(newsemb)).argsort(axis=1)[:, -k:][:, ::-1]]\n",
    "toplabel=user_label_gender[userindex]\n",
    "\n",
    "model.trainable=False\n",
    "\n",
    "topK_input = keras.Input((k,MAX_SENT_LENGTH,), dtype='int32') \n",
    "topKrep=TimeDistributed(news_encoder)(topK_input)\n",
    "\n",
    "topK_score_news=topKrep#concatenate([topK_score_dense,topKrep])\n",
    "\n",
    "\n",
    "attention = Dense(200,activation='tanh')(topKrep)\n",
    "attention = Flatten()(Dense(1)(attention))\n",
    "attention = Activation('softmax')(attention)\n",
    "user_gen_rep=keras.layers.Dot((1, 1))([topKrep, attention])\n",
    "out=Dense(1,activation='sigmoid')(user_gen_rep)\n",
    "usergendermodel = Model([topK_input], [out])\n",
    "usergendermodel.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.001), metrics=['acc'])\n",
    "for _ in range(10):\n",
    "    usergendermodel.fit([topnews[gender_index[:int(0.8*len(gender_index))]]],toplabel[gender_index[:int(0.8*len(gender_index))]],batch_size=64)\n",
    "    pred=usergendermodel.predict([topnews[gender_index[int(0.8*len(gender_index)):]]],batch_size=32)\n",
    "    print(classification_report(toplabel[gender_index[int(0.8*len(gender_index)):]],np.round(pred),digits=4))\n",
    "    print(accuracy_score(toplabel[gender_index[int(0.8*len(gender_index)):]],np.round(pred)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled0.ipynb（副本）（副本）",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
